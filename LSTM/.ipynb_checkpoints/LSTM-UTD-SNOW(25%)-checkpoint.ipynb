{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadf10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef362a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_depth_data(action, subject, trial):\n",
    "    filename = f'../Data/UTDMHAD_data/Depth/a{action}_s{subject}_t{trial}_depth.mat'# The name of the .mat files\n",
    "    if Path(filename).is_file():\n",
    "        mat = scipy.io.loadmat(filename)\n",
    "        return mat['d_depth']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def transform_depth_data(action, subject, trial):\n",
    "    rows = []\n",
    "    data = import_depth_data(action, subject, trial)\n",
    "    if data is None: return None\n",
    "    for frame in range(data.shape[2]):\n",
    "        pixels = data[:, :, frame].flatten()\n",
    "        rows.append(pixels)\n",
    "    result = np.insert(rows, 0, [[action], [subject], [trial], [frame]], axis=1)\n",
    "    return np.array(result)\n",
    "\n",
    "def transform_depth_data_to_df(action, subject, trial):\n",
    "    data = transform_depth_data(action, subject, trial)\n",
    "    if data is None: return None\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['action', 'subject', 'trial', 'frame'] + [f'depth_{n}' for n in range(240 * 320)]\n",
    "    return df\n",
    "\n",
    "def export_depth_data_to_csv(action, subject, trial):\n",
    "    df = transform_depth_data_to_df(action, subject, trial)\n",
    "    if df is None: return None\n",
    "    filename = f'a{action}_s{subject}_t{trial}_depth.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def show_depth_image(action, subject, trial, frame):\n",
    "    data = import_depth_data(action, subject, trial)\n",
    "    if data is None: return None\n",
    "    plt.imshow(data[:,:,frame], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c772c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_inertial_data(action, subject, trial):\n",
    "    filename = f'../Data/UTDMHAD_data/Inertial/a{action}_s{subject}_t{trial}_inertial.mat'\n",
    "    if Path(filename).is_file():\n",
    "        mat = scipy.io.loadmat(filename)\n",
    "        return mat['d_iner']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def transform_inertial_data(action, subject, trial):\n",
    "    data = import_inertial_data(action, subject, trial)\n",
    "    if data is None: return None\n",
    "    result = np.insert(data, 0, [[action], [subject], [trial]], axis=1)\n",
    "    return np.array(result)\n",
    "\n",
    "def transform_inertial_data_to_df(action, subject, trial):\n",
    "    data = transform_inertial_data(action, subject, trial)\n",
    "    if data is None: return None\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['action', 'subject', 'trial', 'x-accel', 'y-accel', 'z-accel', 'x-ang-accel', 'y-ang-accel', 'z-ang-accel']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [i for i in range(1,28)]\n",
    "raw_dataframe = transform_inertial_data_to_df(0, 0, 0)\n",
    "for index, action in enumerate(activities):\n",
    "    for subject in range(1, 9):\n",
    "        for trial in range(1, 5):\n",
    "            data = transform_inertial_data_to_df(action, subject, trial) # (160, 6)\n",
    "            if data is None: continue\n",
    "            #data = data[0:128] #(128.6) # maximum length is 128\n",
    "            raw_dataframe = pd.concat([raw_dataframe, data])\n",
    "raw_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b20983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size: size of time window\n",
    "# step: overlapping\n",
    "# data: dataset\n",
    "def time_windows(window_size,overlapping,data):\n",
    "  sigmentation_data_temp = []\n",
    "  sigmentation_data = []\n",
    "  sigmentation_label = []\n",
    "  for i in range(0,len(data),overlapping):\n",
    "    acc_x = data['x-accel'].values[i:i+window_size]\n",
    "    acc_y = data['y-accel'].values[i:i+window_size]\n",
    "    acc_z = data['z-accel'].values[i:i+window_size]\n",
    "    gyro_x = data['x-ang-accel'].values[i:i+window_size]\n",
    "    gyro_y = data['y-ang-accel'].values[i:i+window_size]\n",
    "    gyro_z = data['z-ang-accel'].values[i:i+window_size]\n",
    "    total_label = data['action'].values[i:i+window_size]\n",
    "    label = Counter(total_label).most_common()[0][0]\n",
    "    sigmentation_data_temp.append([acc_x,acc_y,acc_z,gyro_x,gyro_y,gyro_z])\n",
    "    sigmentation_arr = np.asarray(sigmentation_data_temp)\n",
    "    sig_size = sigmentation_arr.shape\n",
    "    if sig_size[2] == window_size:\n",
    "      sigmentation_arr.reshape(window_size,6)\n",
    "      sigmentation_data.append(sigmentation_arr)\n",
    "      sigmentation_label.append(label)\n",
    "      sigmentation_data_temp = []\n",
    "    else:\n",
    "      sigmentation_data_temp = []\n",
    "\n",
    "  sigmentation_data_arr = np.asarray(sigmentation_data)\n",
    "  sigmentation_label_arr = np.asarray(sigmentation_label)\n",
    "\n",
    "  return sigmentation_data_arr,sigmentation_label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb160ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utd_readings,utd_labels = time_windows(128,128,raw_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8961597",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of data:')\n",
    "print(utd_readings.shape)\n",
    "print('Shape of label')\n",
    "utd_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "utd_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d67908",
   "metadata": {},
   "outputs": [],
   "source": [
    "utd_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset \n",
    "idx_list = np.array(range(2430))\n",
    "np.random.shuffle(idx_list)\n",
    "train_idxes = idx_list[:2000]\n",
    "test_idxes = idx_list[2000:]\n",
    "train_features = utd_readings[train_idxes]\n",
    "train_labels = utd_labels[train_idxes]\n",
    "test_features = utd_readings[test_idxes]\n",
    "test_labels = utd_labels[test_idxes]\n",
    "# train_features = np.array([utd_readings[i] for i in train_idxes])\n",
    "# train_labels = np.array([utd_labels[i] for i in train_idxes])\n",
    "\n",
    "# test_features = np.array([utd_readings[i] for i in test_idxes])\n",
    "# test_labels = np.array([utd_labels[i] for i in test_idxes])\n",
    "# train_features, train_labels = utd_readings, utd_labels\n",
    "# test_features, test_labels = utd_readings, utd_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed75bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(train_features).to(torch.float32),torch.from_numpy(train_labels).to(torch.float32))\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_features).to(torch.float32),torch.from_numpy(test_labels).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f32cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,batch_size = 64,shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,batch_size = 64,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43da715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first = True)\n",
    "        #FC_layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0),self.hidden_dim).requires_grad_()#.to(device) \n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0),self.hidden_dim).requires_grad_()#.to(device) \n",
    "        # X.SIZE = batch SIZE\n",
    "        \n",
    "        # detach the hidden state to prevent exploding gradient\n",
    "        out, (hn,cn) = self.lstm(x,( h0.detach(),c0.detach()))# detach \n",
    "        out = self.fc(out[:, -1, :])# -1 the last layer state at time t     \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec51024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter \n",
    "BATCH_SIZE = 32 # \n",
    "EPOCHES = 10\n",
    "input_dim = 128 # input dimension\n",
    "hidden_dim = 1000 # hidden layers number\n",
    "layer_dim = 1 \n",
    "output_dim = 27 + 1 # output dimension\n",
    "\n",
    "# initialize the model\n",
    "model_lstm = LSTM_model(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else  'cpu')\n",
    "\n",
    "#model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model_lstm.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_dim = 6\n",
    "lost_list_LSTM = []\n",
    "accuracy_list = []\n",
    "iteration_list = [] \n",
    "iter = 0\n",
    "\n",
    "for epoch in range(EPOCHES):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model_lstm.train()\n",
    "        # 32*1*6*128\n",
    "        images = images.view(-1, sequence_dim, input_dim).requires_grad_()#.to(device)\n",
    "        labels = labels.to(torch.int64)\n",
    "        #labels = labels.to(device)\n",
    "        # gradient.zero\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        \n",
    "        outputs = model_lstm(images)\n",
    "        # print(output.size(), labels.size(), \"pass\")\n",
    "        # loss calc\n",
    "        loss_lstm = criterion(outputs, labels)\n",
    "        # backword\n",
    "        loss_lstm.backward()\n",
    "        # renew the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 10 == 0:\n",
    "            model_lstm.eval()\n",
    "            # accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                    images = images.view(-1, sequence_dim, input_dim)#.to(device)\n",
    "                    outputs = model_lstm(images)\n",
    "                    #print(outputs.size())\n",
    "                    \n",
    "                    predict = torch.max(outputs.data, 1)[1]\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predict == labels).sum()\n",
    "            \n",
    "            accuracy = correct / total * 100\n",
    "            lost_list_LSTM.append(loss_lstm.data)\n",
    "            accuracy_list.append(accuracy)\n",
    "            iteration_list.append(iter)\n",
    "            # print the info\n",
    "            print(\"Iter:{},loss:{},Accuracy:{}\".format(iter, loss_lstm.item(), accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "predictions = []\n",
    "classes = [\n",
    "     '1. right arm swipe to the left', \n",
    "    '2. right arm swipe to the right', \n",
    "    '3. right hand wave',\n",
    "    '4. two hand front clap',\n",
    "    '5. right arm throw',\n",
    "    '6. cross arms in the chest',\n",
    "    '7. basketball shoot', \n",
    "    '8. right hand draw x',\n",
    "    '9. right hand draw circle (clockwise)',\n",
    "    '10. right hand draw circle (counter clockwise)',\n",
    "    '11. draw triangle', \n",
    "    '12. bowling (right hand)', \n",
    "    '13. front boxing',\n",
    "    '14. baseball swing from right',\n",
    "    '15. tennis right hand forehand swing',\n",
    "    '16. arm curl (two arms)', \n",
    "    '17. tennis serve', \n",
    "    '18. two hand push',\n",
    "    '19. right hand knock on door', \n",
    "    '20. right hand catch an object',\n",
    "    '21. right hand pick up and throw',\n",
    "    '22. jogging in place',\n",
    "    '23. walking in place',\n",
    "    '24. sit to stand', \n",
    "    '25. stand to sit', \n",
    "    '26. forward lunge (left foot forward',\n",
    "    '27. squat'\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.view(-1, sequence_dim, input_dim)#.to(device)\n",
    "        outputs = model_lstm(images)\n",
    "        #print(outputs.size())\n",
    "                    \n",
    "        predict = torch.max(outputs.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum()\n",
    "        predictions.append(predict)\n",
    "        labels_list.append(labels)\n",
    "    \n",
    "    print('Test Accuracy of the basic LSTM model on the UTD test features: {} %'.format((correct / total) * 100))\n",
    "    \n",
    "mat = metrics.confusion_matrix(torch.cat(predictions), torch.cat(labels_list))\n",
    "\n",
    "plt.figure(figsize=(27, 27))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070d73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe19c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn\n",
    "# num_feature = 6\n",
    "# feature_width = 128\n",
    "class RNN_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        #循环对象\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first = True, nonlinearity = \"relu\")\n",
    "        \n",
    "        #FC_layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0),self.hidden_dim).requires_grad_()#.to(device) \n",
    "        # X.SIZE = batch SIZE\n",
    "        \n",
    "        # 分离隐藏状态，避免梯度爆炸\n",
    "        out, hn = self.rnn(x, h0.detach())# detach 分离\n",
    "        out = self.fc(out[:, -1, :])# -1 the last layer state at time t\n",
    "            \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 128 # input dimension\n",
    "hidden_dim = 100 # hidden layers number \n",
    "layer_dim = 2\n",
    "output_dim = 27 + 1 # output dimension\n",
    "\n",
    "# initialize the model\n",
    "model_rnn = RNN_model(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else  'cpu')\n",
    "\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a693aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model_rnn.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e79ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = (len(list(model_rnn.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e61f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_dim = 6\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "iteration_list = [] \n",
    "# 24576 = 32*1*6*128\n",
    "iter =  0\n",
    "for epoch in range(EPOCHES):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model_rnn.train()\n",
    "        # A batch of data with transfering the RNN input dimention \n",
    "        # 32*1*6*128\n",
    "        images = images.view(-1, sequence_dim, input_dim).requires_grad_()#.to(device)\n",
    "        labels = labels.to(torch.int64)\n",
    "        #labels = labels.to(device)\n",
    "        # gradient.zero\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        output = model_rnn(images)\n",
    "\n",
    "        \n",
    "        # print(output.size(), labels.size(), \"pass\")\n",
    "        # loss calc\n",
    "        loss_rnn = criterion(output, labels)\n",
    "        # backword\n",
    "        loss_rnn.backward()\n",
    "        # renew the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 5 == 0:\n",
    "            model_rnn.eval()\n",
    "            # accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                    images = images.view(-1, sequence_dim, input_dim)#.to(device)\n",
    "                    outputs = model_rnn(images)\n",
    "                    #print(outputs.size())\n",
    "                    \n",
    "                    predict = torch.max(outputs.data, 1)[1]\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predict == labels).sum()\n",
    "            \n",
    "            accuracy = correct / total * 100\n",
    "            loss_list.append(loss_rnn.data)\n",
    "            accuracy_list.append(accuracy)\n",
    "            iteration_list.append(iter)\n",
    "            # print the info\n",
    "            print(\"Iter:{},loss:{},Accuracy:{}\".format(iter, loss_rnn.item(), accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "predictions = []\n",
    "classes = [\n",
    "     '1. right arm swipe to the left', \n",
    "    '2. right arm swipe to the right', \n",
    "    '3. right hand wave',\n",
    "    '4. two hand front clap',\n",
    "    '5. right arm throw',\n",
    "    '6. cross arms in the chest',\n",
    "    '7. basketball shoot', \n",
    "    '8. right hand draw x',\n",
    "    '9. right hand draw circle (clockwise)',\n",
    "    '10. right hand draw circle (counter clockwise)',\n",
    "    '11. draw triangle', \n",
    "    '12. bowling (right hand)', \n",
    "    '13. front boxing',\n",
    "    '14. baseball swing from right',\n",
    "    '15. tennis right hand forehand swing',\n",
    "    '16. arm curl (two arms)', \n",
    "    '17. tennis serve', \n",
    "    '18. two hand push',\n",
    "    '19. right hand knock on door', \n",
    "    '20. right hand catch an object',\n",
    "    '21. right hand pick up and throw',\n",
    "    '22. jogging in place',\n",
    "    '23. walking in place',\n",
    "    '24. sit to stand', \n",
    "    '25. stand to sit', \n",
    "    '26. forward lunge (left foot forward',\n",
    "    '27. squat'\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images = images.view(-1, sequence_dim, input_dim)#.to(device)\n",
    "        outputs = model_rnn(images)\n",
    "        #print(outputs.size())\n",
    "                    \n",
    "        predict = torch.max(outputs.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum()\n",
    "        predictions.append(predict)\n",
    "        labels_list.append(labels)\n",
    "    \n",
    "    print('Test Accuracy of the basic RNN model on the UTD test features: {} %'.format((correct / total) * 100))\n",
    "    \n",
    "mat = metrics.confusion_matrix(torch.cat(predictions), torch.cat(labels_list))\n",
    "\n",
    "plt.figure(figsize=(27, 27))\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
