{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7895199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ced3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_75ol = np.load('../Data/UTDMHAD_data/UTD_train_feature_32wz_75ol.npy')\n",
    "train_label_75ol = np.load('../Data/UTDMHAD_data/UTD_train_label_32wz_75ol.npy')\n",
    "test_feature_75ol = np.load('../Data/UTDMHAD_data/UTD_test_feature_32wz_75ol.npy')\n",
    "test_label_75ol = np.load('../Data/UTDMHAD_data/UTD_test_label_32wz_75ol.npy')\n",
    "\n",
    "train_feature_50ol = np.load('../Data/UTDMHAD_data/UTD_train_feature_32wz_50ol.npy')\n",
    "train_label_50ol = np.load('../Data/UTDMHAD_data/UTD_train_label_32wz_50ol.npy')\n",
    "test_feature_50ol = np.load('../Data/UTDMHAD_data/UTD_test_feature_32wz_50ol.npy')\n",
    "test_label_50ol = np.load('../Data/UTDMHAD_data/UTD_test_label_32wz_50ol.npy')\n",
    "\n",
    "train_feature_25ol = np.load('../Data/UTDMHAD_data/UTD_train_feature_32wz_25ol.npy')\n",
    "train_label_25ol = np.load('../Data/UTDMHAD_data/UTD_train_label_32wz_25ol.npy')\n",
    "test_feature_25ol = np.load('../Data/UTDMHAD_data/UTD_test_feature_32wz_25ol.npy')\n",
    "test_label_25ol = np.load('../Data/UTDMHAD_data/UTD_test_label_32wz_25ol.npy')\n",
    "\n",
    "train_feature_0ol = np.load('../Data/UTDMHAD_data/UTD_train_feature_32wz_0ol.npy')\n",
    "train_label_0ol = np.load('../Data/UTDMHAD_data/UTD_train_label_32wz_0ol.npy')\n",
    "test_feature_0ol = np.load('../Data/UTDMHAD_data/UTD_test_feature_32wz_0ol.npy')\n",
    "test_label_0ol = np.load('../Data/UTDMHAD_data/UTD_test_label_32wz_0ol.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fd43a",
   "metadata": {},
   "source": [
    "# 32 window_size %75 overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908e0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_75ol = TensorDataset(torch.from_numpy(train_feature_75ol).to(torch.float32),torch.from_numpy(train_label_75ol).to(torch.float32))\n",
    "\n",
    "test_dataset_75ol = TensorDataset(torch.from_numpy(test_feature_75ol).to(torch.float32),torch.from_numpy(test_label_75ol).to(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa26f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_75ol = data.DataLoader(dataset = train_dataset_75ol, batch_size = 32, shuffle = True)\n",
    "\n",
    "test_loader_75ol = data.DataLoader(dataset = test_dataset_75ol, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7306257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_UTD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_UTD, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.MaxPool2d(2,2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(3520, 512)\n",
    "        self.fc3 = nn.Linear(512,27)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # out = self.drop_out(out)\n",
    "        out = self.fc1(out)           \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "model_75 = CNN_UTD().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d050d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 20\n",
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_75.parameters(),lr=lr, weight_decay=0.0001,momentum = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f35f69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 accuracy:  22.68086669114947  % Loss:  2.5917501718225613\n",
      "Epoch:  2 accuracy:  34.21226588321704  % Loss:  2.096889034403322\n",
      "Epoch:  3 accuracy:  40.543518178479616  % Loss:  1.8769991397857666\n",
      "Epoch:  4 accuracy:  45.053250091810504  % Loss:  1.7221473768843172\n",
      "Epoch:  5 accuracy:  48.703635695923616  % Loss:  1.611051515794136\n",
      "Epoch:  6 accuracy:  50.811604847594566  % Loss:  1.5192642760388728\n",
      "Epoch:  7 accuracy:  53.77892030848329  % Loss:  1.4389125813900585\n",
      "Epoch:  8 accuracy:  55.92361366140286  % Loss:  1.3580042469389562\n",
      "Epoch:  9 accuracy:  58.222548659566655  % Loss:  1.2938282737429714\n",
      "Epoch:  10 accuracy:  60.44069041498347  % Loss:  1.214375977365064\n",
      "Epoch:  11 accuracy:  62.39441792141021  % Loss:  1.1557284765680071\n",
      "Epoch:  12 accuracy:  64.32611090708777  % Loss:  1.0977607067202177\n",
      "Epoch:  13 accuracy:  66.24311421226588  % Loss:  1.0556696654065674\n",
      "Epoch:  14 accuracy:  67.76349614395887  % Loss:  0.9877359427056962\n",
      "Epoch:  15 accuracy:  69.28387807565186  % Loss:  0.9463672250369345\n",
      "Epoch:  16 accuracy:  71.27432978332722  % Loss:  0.8878627149431918\n",
      "Epoch:  17 accuracy:  71.69298567756151  % Loss:  0.8607659665211825\n",
      "Epoch:  18 accuracy:  73.11053984575835  % Loss:  0.8221198705860147\n",
      "Epoch:  19 accuracy:  74.01395519647447  % Loss:  0.7882047232607721\n",
      "Epoch:  20 accuracy:  74.8879911861917  % Loss:  0.7523168821429982\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "loss_list = []\n",
    "for epoch in range(epoches):\n",
    "    acc = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_num = 0\n",
    "    runtime_loss = 0\n",
    "    for (imgs,labels) in train_loader_75ol:\n",
    "        imgs = imgs.reshape(len(imgs),1,32,6)\n",
    "#         imgs = imgs.reshape(len(imgs),1,16,24)     \n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_75(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        runtime_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        loss_num+=1\n",
    "    acc = 100 * correct / total\n",
    "    epoch_loss = runtime_loss / loss_num\n",
    "    loss_list.append(epoch_loss)\n",
    "    loss_num = 0\n",
    "    print(\"Epoch: \",epoch+1, \"accuracy: \", acc,' %',\"Loss: \",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e192f7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = []\n",
    "for i in range(1,21):\n",
    "  x.append(i)\n",
    "y = loss_list\n",
    "fig = plt.figure()\n",
    "plt.plot(x,y,color = 'blue')\n",
    "plt.title('training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17d256c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy is:  49.24605894448252\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "correct_test = 0\n",
    "\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for (imgs_test,labels_test) in test_loader_75ol:\n",
    "\n",
    "    imgs_test = imgs_test.reshape(len(imgs_test),1,32,6)\n",
    "\n",
    "    imgs_test = imgs_test.to(device)\n",
    "\n",
    "    labels_test = labels_test.type(torch.LongTensor)\n",
    "\n",
    "    labels_test = labels_test.to(device)\n",
    "\n",
    "    outputs_test = model_75(imgs_test)\n",
    "\n",
    "    labels_test = labels_test.reshape(labels_test.shape[0],)\n",
    "\n",
    "    predicted_test = torch.max(outputs_test.data,1)[1]\n",
    "      \n",
    "    total_test += labels_test.size(0)\n",
    "      \n",
    "    correct_test += (predicted_test == labels_test).sum().item()\n",
    "\n",
    "    test_accuracy = 100*correct_test/total_test\n",
    "\n",
    "print('validation accuracy is: ',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28026111",
   "metadata": {},
   "source": [
    "# 32 window_size %50 overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66794c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_50ol = TensorDataset(torch.from_numpy(train_feature_50ol).to(torch.float32),torch.from_numpy(train_label_50ol).to(torch.float32))\n",
    "\n",
    "test_dataset_50ol = TensorDataset(torch.from_numpy(test_feature_50ol).to(torch.float32),torch.from_numpy(test_label_50ol).to(torch.float32))\n",
    "\n",
    "train_loader_50ol = data.DataLoader(dataset = train_dataset_50ol, batch_size = 32, shuffle = True)\n",
    "\n",
    "test_loader_50ol = data.DataLoader(dataset = test_dataset_50ol, batch_size = 32, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf5ec4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_UTD_50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_UTD_50, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.MaxPool2d(2,2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(3520, 512)\n",
    "        self.fc3 = nn.Linear(512,27)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # out = self.drop_out(out)\n",
    "        out = self.fc1(out)           \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "model_50 = CNN_UTD_50().to(device)\n",
    "epoches = 20\n",
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_50.parameters(),lr=lr, weight_decay=0.0001,momentum = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7f5bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 accuracy:  18.27262044653349  % Loss:  2.814808272419961\n",
      "Epoch:  2 accuracy:  29.832549941245592  % Loss:  2.316816548786253\n",
      "Epoch:  3 accuracy:  35.237955346651  % Loss:  2.0841485972695506\n",
      "Epoch:  4 accuracy:  39.60047003525264  % Loss:  1.9283271294804247\n",
      "Epoch:  5 accuracy:  43.06698002350176  % Loss:  1.784101492362403\n",
      "Epoch:  6 accuracy:  46.46004700352526  % Loss:  1.687443752803713\n",
      "Epoch:  7 accuracy:  49.26556991774383  % Loss:  1.573340012993611\n",
      "Epoch:  8 accuracy:  52.291421856639246  % Loss:  1.467766329156401\n",
      "Epoch:  9 accuracy:  55.15569917743831  % Loss:  1.3802575018484269\n",
      "Epoch:  10 accuracy:  57.16803760282021  % Loss:  1.3275889317754288\n",
      "Epoch:  11 accuracy:  60.370152761457106  % Loss:  1.222214077839829\n",
      "Epoch:  12 accuracy:  62.39717978848414  % Loss:  1.156755029035846\n",
      "Epoch:  13 accuracy:  64.21856639247943  % Loss:  1.1036214741742667\n",
      "Epoch:  14 accuracy:  66.42185663924795  % Loss:  1.03053583309684\n",
      "Epoch:  15 accuracy:  68.66921269095182  % Loss:  0.961348166488146\n",
      "Epoch:  16 accuracy:  70.15276145710928  % Loss:  0.9019642682142661\n",
      "Epoch:  17 accuracy:  72.16509988249119  % Loss:  0.8578524074643991\n",
      "Epoch:  18 accuracy:  73.57520564042304  % Loss:  0.8193247439995618\n",
      "Epoch:  19 accuracy:  74.39776733254995  % Loss:  0.7665209465183562\n",
      "Epoch:  20 accuracy:  76.14571092831963  % Loss:  0.737760520457102\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "loss_list = []\n",
    "for epoch in range(epoches):\n",
    "    acc = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_num = 0\n",
    "    runtime_loss = 0\n",
    "    for (imgs,labels) in train_loader_50ol:\n",
    "        imgs = imgs.reshape(len(imgs),1,32,6)\n",
    "#         imgs = imgs.reshape(len(imgs),1,16,24)     \n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_50(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        runtime_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        loss_num+=1\n",
    "    acc = 100 * correct / total\n",
    "    epoch_loss = runtime_loss / loss_num\n",
    "    loss_list.append(epoch_loss)\n",
    "    loss_num = 0\n",
    "    print(\"Epoch: \",epoch+1, \"accuracy: \", acc,' %',\"Loss: \",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87632d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy is:  41.94653872515421\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "correct_test = 0\n",
    "\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for (imgs_test,labels_test) in test_loader_50ol:\n",
    "\n",
    "    imgs_test = imgs_test.reshape(len(imgs_test),1,32,6)\n",
    "\n",
    "    imgs_test = imgs_test.to(device)\n",
    "\n",
    "    labels_test = labels_test.type(torch.LongTensor)\n",
    "\n",
    "    labels_test = labels_test.to(device)\n",
    "\n",
    "    outputs_test = model_50(imgs_test)\n",
    "\n",
    "    labels_test = labels_test.reshape(labels_test.shape[0],)\n",
    "\n",
    "    predicted_test = torch.max(outputs_test.data,1)[1]\n",
    "      \n",
    "    total_test += labels_test.size(0)\n",
    "      \n",
    "    correct_test += (predicted_test == labels_test).sum().item()\n",
    "\n",
    "    test_accuracy = 100*correct_test/total_test\n",
    "\n",
    "print('validation accuracy is: ',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef034a9",
   "metadata": {},
   "source": [
    "# 32 window_size %25 overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb611c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_25ol = TensorDataset(torch.from_numpy(train_feature_25ol).to(torch.float32),torch.from_numpy(train_label_25ol).to(torch.float32))\n",
    "\n",
    "test_dataset_25ol = TensorDataset(torch.from_numpy(test_feature_25ol).to(torch.float32),torch.from_numpy(test_label_25ol).to(torch.float32))\n",
    "\n",
    "train_loader_25ol = data.DataLoader(dataset = train_dataset_25ol, batch_size = 32, shuffle = True)\n",
    "\n",
    "test_loader_25ol = data.DataLoader(dataset = test_dataset_25ol, batch_size = 32, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86e4b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_UTD_25(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_UTD_25, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.MaxPool2d(2,2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(3520, 512)\n",
    "        self.fc3 = nn.Linear(512,27)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # out = self.drop_out(out)\n",
    "        out = self.fc1(out)           \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "model_25 = CNN_UTD_25().to(device)\n",
    "epoches = 20\n",
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_25.parameters(),lr=lr, weight_decay=0.0001,momentum = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab6bf48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 accuracy:  14.125165271044512  % Loss:  2.9484081201150385\n",
      "Epoch:  2 accuracy:  25.187307183781403  % Loss:  2.5135164848515683\n",
      "Epoch:  3 accuracy:  31.35742617893345  % Loss:  2.277346685738631\n",
      "Epoch:  4 accuracy:  36.40370207139709  % Loss:  2.0860377902715976\n",
      "Epoch:  5 accuracy:  39.40061701189951  % Loss:  1.9565894293113493\n",
      "Epoch:  6 accuracy:  41.780520052886736  % Loss:  1.8349828929968284\n",
      "Epoch:  7 accuracy:  45.813133539003964  % Loss:  1.7082097983696092\n",
      "Epoch:  8 accuracy:  48.72190392243279  % Loss:  1.6211552485613756\n",
      "Epoch:  9 accuracy:  51.36624063464081  % Loss:  1.5055127362130394\n",
      "Epoch:  10 accuracy:  54.58351696782724  % Loss:  1.4271448724706408\n",
      "Epoch:  11 accuracy:  57.88893785808726  % Loss:  1.3269594991710825\n",
      "Epoch:  12 accuracy:  60.709563684442486  % Loss:  1.232490964758564\n",
      "Epoch:  13 accuracy:  62.9352137505509  % Loss:  1.1614797241251233\n",
      "Epoch:  14 accuracy:  65.35918907007492  % Loss:  1.0855275707345613\n",
      "Epoch:  15 accuracy:  67.87130894667254  % Loss:  1.0036571894733\n",
      "Epoch:  16 accuracy:  69.21551344204495  % Loss:  0.9541447380898704\n",
      "Epoch:  17 accuracy:  71.74966945791097  % Loss:  0.8533778446660915\n",
      "Epoch:  18 accuracy:  74.4380784486558  % Loss:  0.7859630269903533\n",
      "Epoch:  19 accuracy:  74.5923314235346  % Loss:  0.7728694538835069\n",
      "Epoch:  20 accuracy:  77.19259585720582  % Loss:  0.7099466531629294\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "loss_list = []\n",
    "for epoch in range(epoches):\n",
    "    acc = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_num = 0\n",
    "    runtime_loss = 0\n",
    "    for (imgs,labels) in train_loader_25ol:\n",
    "        imgs = imgs.reshape(len(imgs),1,32,6)\n",
    "#         imgs = imgs.reshape(len(imgs),1,16,24)     \n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_25(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        runtime_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        loss_num+=1\n",
    "    acc = 100 * correct / total\n",
    "    epoch_loss = runtime_loss / loss_num\n",
    "    loss_list.append(epoch_loss)\n",
    "    loss_num = 0\n",
    "    print(\"Epoch: \",epoch+1, \"accuracy: \", acc,' %',\"Loss: \",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac93a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy is:  41.67523124357657\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "correct_test = 0\n",
    "\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for (imgs_test,labels_test) in test_loader_25ol:\n",
    "\n",
    "    imgs_test = imgs_test.reshape(len(imgs_test),1,32,6)\n",
    "\n",
    "    imgs_test = imgs_test.to(device)\n",
    "\n",
    "    labels_test = labels_test.type(torch.LongTensor)\n",
    "\n",
    "    labels_test = labels_test.to(device)\n",
    "\n",
    "    outputs_test = model_25(imgs_test)\n",
    "\n",
    "    labels_test = labels_test.reshape(labels_test.shape[0],)\n",
    "\n",
    "    predicted_test = torch.max(outputs_test.data,1)[1]\n",
    "      \n",
    "    total_test += labels_test.size(0)\n",
    "      \n",
    "    correct_test += (predicted_test == labels_test).sum().item()\n",
    "\n",
    "    test_accuracy = 100*correct_test/total_test\n",
    "\n",
    "print('validation accuracy is: ',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0aed8",
   "metadata": {},
   "source": [
    "# 32 window_size %0 overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff6fabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_0ol = TensorDataset(torch.from_numpy(train_feature_0ol).to(torch.float32),torch.from_numpy(train_label_0ol).to(torch.float32))\n",
    "\n",
    "test_dataset_0ol = TensorDataset(torch.from_numpy(test_feature_0ol).to(torch.float32),torch.from_numpy(test_label_0ol).to(torch.float32))\n",
    "\n",
    "train_loader_0ol = data.DataLoader(dataset = train_dataset_0ol, batch_size = 32, shuffle = True)\n",
    "\n",
    "test_loader_0ol = data.DataLoader(dataset = test_dataset_0ol, batch_size = 32, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dd958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_UTD_0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_UTD_0, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.MaxPool2d(2,2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(3520, 512)\n",
    "        self.fc3 = nn.Linear(512,27)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # out = self.drop_out(out)\n",
    "        out = self.fc1(out)           \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "model_0 = CNN_UTD_0().to(device)\n",
    "epoches = 20\n",
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_0.parameters(),lr=lr, weight_decay=0.0001,momentum = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d9b81dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 accuracy:  12.72032902467685  % Loss:  3.0119180211396976\n",
      "Epoch:  2 accuracy:  23.707403055229143  % Loss:  2.591802924592918\n",
      "Epoch:  3 accuracy:  28.936545240893068  % Loss:  2.356608413090216\n",
      "Epoch:  4 accuracy:  33.07873090481786  % Loss:  2.1977288767556162\n",
      "Epoch:  5 accuracy:  36.28084606345476  % Loss:  2.060564413248936\n",
      "Epoch:  6 accuracy:  40.4524089306698  % Loss:  1.9142756239276066\n",
      "Epoch:  7 accuracy:  42.53819036427732  % Loss:  1.8108625723936846\n",
      "Epoch:  8 accuracy:  45.50528789659224  % Loss:  1.7202065358652132\n",
      "Epoch:  9 accuracy:  48.413631022326676  % Loss:  1.6279930121430726\n",
      "Epoch:  10 accuracy:  51.93889541715629  % Loss:  1.5045146351662753\n",
      "Epoch:  11 accuracy:  55.25851938895417  % Loss:  1.4044241816083962\n",
      "Epoch:  12 accuracy:  56.786133960047  % Loss:  1.3264305335338984\n",
      "Epoch:  13 accuracy:  60.54641598119859  % Loss:  1.2323707119326726\n",
      "Epoch:  14 accuracy:  62.54406580493537  % Loss:  1.159917518914303\n",
      "Epoch:  15 accuracy:  65.951821386604  % Loss:  1.0741298209841006\n",
      "Epoch:  16 accuracy:  68.09635722679201  % Loss:  1.0012527370007238\n",
      "Epoch:  17 accuracy:  70.53466509988249  % Loss:  0.9473013334742216\n",
      "Epoch:  18 accuracy:  72.76733254994124  % Loss:  0.8514675921368822\n",
      "Epoch:  19 accuracy:  74.41245593419507  % Loss:  0.8031150617889155\n",
      "Epoch:  20 accuracy:  74.14806110458284  % Loss:  0.7853910229473471\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "loss_list = []\n",
    "for epoch in range(epoches):\n",
    "    acc = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_num = 0\n",
    "    runtime_loss = 0\n",
    "    for (imgs,labels) in train_loader_0ol:\n",
    "        imgs = imgs.reshape(len(imgs),1,32,6)\n",
    "#         imgs = imgs.reshape(len(imgs),1,16,24)     \n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_0(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        runtime_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        loss_num+=1\n",
    "    acc = 100 * correct / total\n",
    "    epoch_loss = runtime_loss / loss_num\n",
    "    loss_list.append(epoch_loss)\n",
    "    loss_num = 0\n",
    "    print(\"Epoch: \",epoch+1, \"accuracy: \", acc,' %',\"Loss: \",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe8e3f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy is:  38.24537354352296\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "correct_test = 0\n",
    "\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for (imgs_test,labels_test) in test_loader_0ol:\n",
    "\n",
    "    imgs_test = imgs_test.reshape(len(imgs_test),1,32,6)\n",
    "\n",
    "    imgs_test = imgs_test.to(device)\n",
    "\n",
    "    labels_test = labels_test.type(torch.LongTensor)\n",
    "\n",
    "    labels_test = labels_test.to(device)\n",
    "\n",
    "    outputs_test = model_0(imgs_test)\n",
    "\n",
    "    labels_test = labels_test.reshape(labels_test.shape[0],)\n",
    "\n",
    "    predicted_test = torch.max(outputs_test.data,1)[1]\n",
    "      \n",
    "    total_test += labels_test.size(0)\n",
    "      \n",
    "    correct_test += (predicted_test == labels_test).sum().item()\n",
    "\n",
    "    test_accuracy = 100*correct_test/total_test\n",
    "\n",
    "print('validation accuracy is: ',test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
